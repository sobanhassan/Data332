---
title: "Chapter16&17Answers"
author: "Soban Hassan"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r  results='hide', warning=FALSE, message=FALSE}
# Prerequisites (Packages & Libraries)

lapply(c("tidyverse", "ggrepel", "nycflights13", "ggthemes", "ggplot2", "rlang", "forcats", "gt", "naniar", "gtExtras", "babynames", "janitor")
       , library, character.only = TRUE)


#library(tidyverse)
#library(ggrepel)
#library(nycflights13)
#library(ggthemes)
#library(ggplot2)
#library(rlang)
#library(forcats)
#library(gt)
```

```{r warning=FALSE}
## 16.3.1 Exercises

#1. Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?

# The default bar chart is hard to understand because: --

# a.It is vertical, and the names of categories overlap on x-axis.

# b.The "Not applicable" category is before the lowest income group. Thus, the pattern is disturbed.

# We could improve the plot, as shown in below, by:

# a.Making it into a horizontal bar chart to allow space and easy reading of categories of income levels.

# b.Move the "Not Applicable" level after the highest income level, along-side "Refused", "Dont' know" and "No answer".

# c.Further, we could remove non-data ink, as per principles of Mr. Tufte to make our pattern stand out. Also, we could create a separate colouring scheme for data outside the income levels.

no_levels = levels(gss_cat$rincome)[c(1:3, 16)]

gss_cat |>
  mutate(col_level = rincome %in% no_levels) |>
  ggplot(aes(y = fct_relevel(rincome, 
                             "Not applicable",
                             after = 3),
             fill = col_level)) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Number of respondents", y = NULL,
       title = "Income Levels of respondents in General Social Survey") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  scale_fill_manual(values = c("#3d3b3b", "#999494"))

#2. What is the most common relig in this survey? What’s the most common partyid?

# The most common `relig` is "Protestant". And, the most common `partyid` is "Independent".

gss_cat |>
  count(relig, sort = TRUE)

gss_cat |>
  count(partyid, sort = TRUE)

#3. Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?

# We can see from the code below that more than one factor values in `denom` (denomination) occur only in "Protestant", "Christian" and "Other" religions. 

gss_cat |>
  group_by(relig) |>
  summarise(n = n_distinct(denom)) |>
  arrange(desc(n)) |>
  filter(n > 1)

# To explore further, we can cross-tabulate religion and denomination, as shown in graph, and realize that the only religion to which denomination really applies to is "Protestant".

gss_cat |>
  filter(relig %in% c("Protestant", "Christian", "Other")) |>
  group_by(relig, denom) |>
  tally() |>
  spread(relig, n) |>
  arrange(desc(Christian)) |>
  gt() |>
  sub_missing(missing_text = "") |>
  gt_theme_538()

## 16.4.1 Exercises

#1. There are some suspiciously high numbers in tvhours. Is the mean a good summary?

# No, mean is not a good summary as the distribution of `tvhours` is right skewed. Instead, we should use median as a summary measure.

gss_cat |>
  drop_na() |>
  mutate(tvhours = as_factor(tvhours)) |>
  ggplot(aes(x = tvhours)) +
  geom_bar(col = "black", fill = "white") +
  theme_clean() +
  labs(x = "Hours per day spent watching TV",
       y = "Numbers", title = "Distribution of TV Hours is right skewed")

#2. For each factor in gss_cat identify whether the order of the levels is arbitrary or principled.

library(tidyverse)
library(forcats)

# Create the final table
factor_order_table <- tribble(
  ~Factor,     ~Description,                        ~`Order Type`,   ~Reasoning,
  "marital",   "Marital status",                    "Arbitrary",     "No inherent order between \"Married\", \"Divorced\", etc.",
  "race",      "Race of respondent",                "Arbitrary",     "Categories are distinct with no ranked order.",
  "relig",     "Religion",                          "Arbitrary",     "Different faiths are listed without hierarchy.",
  "denom",     "Denomination within religion",      "Arbitrary",     "Subcategories of religion, no natural order.",
  "partyid",   "Political party identification",    "Principled",    "Arranged from \"Strong Democrat\" to \"Strong Republican\"—ideological scale.",
  "rincome",   "Respondent income",                 "Principled",    "Ordered by income brackets.",
  "degree",    "Education level",                   "Principled",    "Follows a logical educational progression (e.g., \"Less than HS\" to \"Graduate\").",
  "tvhours",   "Hours of TV watched (numeric, not factor)", "—",     "Not a factor variable.",
  "age",       "Age (numeric)",                     "—",             "Not a factor.",
  "sex",       "Gender",                            "Arbitrary",     "Binary/dichotomous but not ranked."
)

print(factor_order_table)

#3. Why did moving “Not applicable” to the front of the levels move it to the bottom of the plot?
# Moving "Not applicable" to the front of the levels move it to the bottom of the plot, because `ggplot2` plots the levels in increasing order, starting bottom's upwards.

## 16.5.1 Exercises

#1. How have the proportions of people identifying as Democrat, Republican, and Independent changed over time?

# As shown below, the proportions of people identifying as Democrat has slightly increased, Republican has slightly decreased, and Independent has increased, over the period of 15 years reflected in the data-set.

gss_cat |>
  mutate(
    partyid = fct_collapse(partyid,
      "Republican"  = c("Strong republican",  "Not str republican"),
      "Democrat"    = c("Strong democrat", "Not str democrat"),
      "Independent" = c("Independent", "Ind,near dem", "Ind,near rep"),
      "Others"      = c("No answer", "Don't know", "Other party")
    )
  ) |>
  group_by(year, partyid) |>
  count() |>
  ggplot(aes(x = year, y = n, fill = partyid)) +
  geom_col(position = "fill") +
  scale_fill_manual(values = c("lightgrey", "red", "grey", "blue")) +
  theme_classic() +
  theme(legend.position = "bottom") +
  labs(x = "Year", y = "Proportion of respondents", fill = "Party",
       subtitle = "Proportion of republicans has decreased, while that of independents has increased over the years",
       title = "In 15 years, share of parties' supporters has changed")

#2. How could you collapse rincome into a small set of categories?

# We could collapse the `rincome` into a small set of categories using the following functions:

# `fct_lump_n()`, `fct_lump_lowfreq()`, `fct_lump_min()`, `fct_lump_prop()`, `fct_lump()`, `fct_collapse()`

gss_cat|>
  mutate(
    rincome = fct_lump_n(rincome, n = 6)
  ) |>
  group_by(rincome) |>
  count() |>
  arrange(desc(n)) |>
  ungroup() |>
  gt() |>
  cols_label(rincome = "Annual Income",
             n = "Numbers")

#3. Notice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)

# Yes, there are 9 groups (excluding other) in this example, as shown below. This is because `n = 10` argument limits the total groups to 10, and the function needs one group for "Other", i.e. all other groups whose count is lesser than top 9 groups. Thus, the groups shown are 9, with 1 as "Other" (at the end).

gss_cat |>
  mutate(relig = fct_lump_n(relig, n = 10)) |>
  count(relig) |>
  gt()

## 17.2.5 Exercises

#1. What happens if you parse a string that contains invalid dates?

ymd(c("2010-10-10", "bananas"))

# Whenever a string is parsed, that contains invalid dates, a missing value, i.e., `NA` will be generated.

#2. What does the tzone argument to today() do? Why is it important?
# The `tzone` argument aloows us to write "a character vector specifying which time zone you would like the current time in. `tzone` defaults to your computer's system timezone.

# Thus, we can find the date at the current moment in local time zone of the computer system with `today()` and at any other timezone, say UTC, with `today(tzone = "UTC")` . It is important when analyzing real-time data, or data from multiples locations across the globe, as the date may not be the same at all places at all times.

# A more important role of `tzone` is with `now()` as time is different at different zones.

today()
today(tzone = "UTC")

now()
now(tzone = "UTC")

#3. For each of the following date-times, show how you’d parse it using a readr column specification and a lubridate function.

d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
t1 <- "1705"
t2 <- "11:15:10.12 PM"

df = tibble(d1, d2, d3, d4, d5, t1, t2) |>
  slice(1)

df |>
  mutate(
    d1 = mdy(d1),
    d2 = ymd(d2),
    d3 = dmy(d3),
    d4 = mdy(d4),
    d5 = mdy(d5),
    t1 = hm(paste0(as.numeric(t1) %/% 100, ":", as.numeric(t1) %% 100)),
    t2 = hms(t2)
  )

## 17.3.4 Exercises

#1. How does the distribution of flight times within a day change over the course of the year?

# The distribution of flight times within a day change over the course of the year is displayed below. 

flights |>
  mutate(
    dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),
    dep_day = round_date(dep_time, unit = "day")
  ) |>
  group_by(dep_day) |>
  summarise(mean_air_time = mean(air_time, na.rm = TRUE)) |>
  ggplot(aes(x = dep_day,
             y = mean_air_time)) +
  geom_point() +
  geom_smooth(span = 0.5) +
  theme_minimal() +
  labs(y = "Average flight time for flights departing each day (in min)",
       x = NULL,
       title = "The average flight time increases towards the winter months",
       subtitle = "The lowest average flight times occur in late summer and early fall; while highest occur in December",
       caption = "Each dot represents the mean air time for flights departing in that day") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

# To check whether this is due to only some extraordinarily delayed flights, or due to longer flight times in general, we can plot the median, instead of mean, as shown in below. Both plots provide the same conclusion, as explained there-in.

flights |>
  mutate(
    dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),
    dep_day = round_date(dep_time, unit = "day")
  ) |>
  group_by(dep_day) |>
  summarise(median_air_time = median(air_time, na.rm = TRUE)) |>
  ggplot(aes(x = dep_day,
             y = median_air_time)) +
  geom_point() +
  geom_smooth(span = 0.5) +
  theme_minimal() +
  labs(y = "Median flight time for flights departing each day (in min)",
       x = NULL,
       title = "The median flight time increases towards the winter months",
       subtitle = "The lowest median flight times occur in late summer and early fall; while highest occur in December",
       caption = "Each dot represents the median air time for flights departing in that day") +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())

#2. Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.

f1 = flights |>
  select(-hour, -minute, -time_hour) |>
  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(sched_dep_time)) |>
  mutate(
   dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),
   sched_dep_time = make_datetime(year, month, day, sched_dep_time %/% 100, sched_dep_time %% 100),
   dep_delay_calc = dep_time - sched_dep_time,
   comparison = dep_delay_calc/60 == dep_delay,
   .keep = "used"
  )

f1 |>
  slice_head(n = 5) |>
  gt() |>
  gt_theme_538()

# Now, when we compute the percentage of observations where `dep_delay == dep_delay_calc` , we get only 99.63%. This means that for 0.36% of flights, the calculation doesn't match. We need to explore these further.

mean(f1$comparison) * 100

## Code Part 1
f1 |>
  filter(!comparison)

## Code Part 2
f1 |>
  filter(!comparison) |>
  filter(dep_delay_calc > 0)

# Now, after seeing results of `## Code Part 1`, we realize that some flights are so delayed that their scheduled departure time goes over to the next day, and this, unfortunately we did not figure in our calculations. To further confirm, using `## Code Part 2` , we confirm that the only mismatches are the ones where calculated departure delay is negative.

# Hence, we need to re-work our calculations, as follows: ---

## Create new tibble from flights with datetime columns
f2 = flights |>
  select(-hour, -minute, -time_hour) |>
  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(sched_dep_time)) |>
  mutate(
   dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),
   sched_dep_time = make_datetime(year, month, day, sched_dep_time %/% 100, sched_dep_time %% 100)
   )

## Check range of dep_delay: to understand how early can a flight depart from scheduled time
## The earliest is -43 min, i.e., 43 min early

range(f2$dep_delay)

## Now, change the departure time into +1 day, if the dep_time is earlier than scheduled time 
## by more than 45 min
f2 = f2 |>
  mutate(
   dep_time = if_else(dep_time < sched_dep_time - minutes(45),
                      true = dep_time + days(1),
                      false = dep_time),
   dep_delay_calc = dep_time - sched_dep_time,
   dep_delay_calc = dep_delay_calc/60,
   comparison = dep_delay_calc == dep_delay
  )

## Percentage of cases where our calculated departure delay is exactly same as dep_delay column
mean(f2$comparison) * 100

# Thus, we realize that our 100% of our calculations match, once we factor in the flights departing the next day from their scheduled date as reflected in @tbl-q2a-ex3.

## Show some flights delayed so much that they depart the next day, to see comparison
f2 |>
  filter(day(dep_time) != day(sched_dep_time)) |>
  select(year, month, day, dep_time, sched_dep_time, 
         carrier, tailnum, dep_delay, dep_delay_calc, 
         comparison) |>
  slice_head(n = 5) |>
  gt() |>
  gt_theme_538()

#3. Compare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.)

# If we compare the `air_time` with duration between departure time and arrival time, we get hardly 0.06% matches, even after adjusting for `dep_times` that occur on the next day, or arrival times occurring on the next day. This means that something else is going on, perhaps the `origin` and `dest` airports are in different time zones.


f3 = flights |>
  select(-hour, -minute, -time_hour) |>
  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(air_time)) |>
  mutate(
   dep_time = make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100),
   sched_dep_time = make_datetime(year, month, day, sched_dep_time %/% 100, sched_dep_time %% 100),
   dep_time = if_else(dep_time < sched_dep_time - minutes(45),
                      true = dep_time + days(1),
                      false = dep_time),
   arr_time = make_datetime(year, month, day, arr_time %/% 100, arr_time %% 100),
   sched_arr_time = make_datetime(year, month, day, sched_arr_time %/% 100, sched_arr_time %% 100),
   air_time_calc = (arr_time - dep_time),
   comparison = air_time_calc == air_time
  )

f3 |>
  select(c(dep_time,
           sched_dep_time,
           arr_time,
           sched_arr_time,
           origin,
           dest,
           air_time,
           air_time_calc,
           comparison)) |>
  slice(200:205) |>
  gt() |>
  gt_theme_nytimes() |>
  tab_style(
    style = list(cell_text(weight = "bold")),
    locations = cells_body(columns = comparison)
  )

mean(f3$comparison) * 100

# Let's try to add the time-zones of destination airports, and then calculate the flight times. As we see below, the results still don't match. Perhaps `air_time` excludes the time between `dep_time` and `arr_time` spent on the tarmac, runway etc. I hope to return to this analysis later sometime.

# Load data from airports which contains their time zones

data("airports")

# Left join the data set
flights_airports = left_join(flights, airports, by = join_by(dest == faa)) |>
  select(-hour, -minute, -time_hour, -carrier, -tailnum, -lat, -lon, -alt) |>
  filter(!is.na(dep_time) & !is.na(arr_time) & !is.na(air_time))

f4 = flights_airports |>
  select(year, month, day, dep_time, sched_dep_time, dep_delay, 
         arr_time, sched_arr_time, arr_delay,
         flight, origin, dest,air_time, distance, name, tzone) |>
  mutate(
    dep_time = make_datetime(year, month, day, 
                             dep_time %/% 100, dep_time %% 100, 
                             tz = "America/New_York"),
    arr_time = make_datetime(year, month, day, 
                             arr_time %/% 100, arr_time %% 100, 
                             tz = tzone)
    )

f4 |>
  slice(202) |>
  mutate(
    air_time = air_time,
    tzone = tzone,
    air_time_calc = interval(dep_time, arr_time)/minutes(1),
    .keep = "used"
  )

#4. How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why?

# The average delay time increases over the course of a given day. The plot below shows us that peak delays occur in the late evening.

# In my view, we should use scheduled departure time because people arrive at the airport, and plan their travel as per scheduled departure time, and thus, information about scheduled departure time will be more useful to the consumers of the data visualization.

flights |>
  group_by(sched_dep_time) |>
  summarise(
    mean_dep_delay = mean(dep_delay, na.rm = TRUE),
    median_dep_delay = median(dep_delay, na.rm = TRUE)
  ) |>
  mutate(
    sched_dep_hour = sched_dep_time %/% 100,
    sched_dep_min = sched_dep_time %% 100,
    sched_dep_time = sched_dep_hour + (sched_dep_min/60)
  ) |>
  
  ggplot(aes(x = sched_dep_time,
             y = mean_dep_delay)) +
  geom_point(size = 0.2) +
  geom_line() +
  geom_smooth(col = "red") +
  coord_cartesian(xlim = c(4, 24),
                  ylim = c(0,80)) +
  scale_x_continuous(breaks = seq(5,24,1)) +
  theme_minimal() +
  labs(x = "Scheduled Depature Time during the day",
       y = "Average Departure Delay (minutes)",
       title = "Average departure delay for flights rises as the day progresses",
       subtitle = "Flights in the evening, 6 pm to 10 pm, have the highest departure delays\nConversely, early morning flights have minimal delays") +
   theme(panel.grid.minor.x = element_blank(),
         plot.title.position = "plot")

#5. On what day of the week should you leave if you want to minimise the chance of a delay?

# As we can see below, if we want to minimize the chance of delay, we should leave on Saturday. Since the questions uses "should you leave", we use `dep_time` in place of `sched_dep_time` .

labels_grid = c(
  mean_delay = "Mean delay (in mins)",
  median_delay = "Median delay (in mins)",
  numbers = "Number of flights")

flights |>
  mutate(
   dep_time = make_datetime(year, month, day,
                            dep_time %/% 100,
                            dep_time %% 100),
   weekday = wday(dep_time, label = TRUE, abbr = FALSE)
  ) |>
  group_by(weekday) |>
  summarise(
    mean_delay = mean(dep_delay, na.rm = TRUE),
    median_delay = median(dep_delay, na.rm = TRUE),
    numbers = n()
  ) |>
  drop_na() |>
  pivot_longer(cols = -weekday,
               names_to = "indicator",
               values_to = "val") |>
  ggplot(aes(y = weekday, x = val, fill = indicator)) +
  geom_bar(stat = "identity") +
  geom_vline(xintercept = 0) +
  facet_grid(~ indicator, scales = "free_x",
             labeller = labeller(indicator = labels_grid)) +
  labs(x = NULL, y = NULL) + 
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Dark2")

#6. What makes the distribution of diamonds$carat and flights$sched_dep_time similar?

# The distribution of `diamonds$carat` and `flights$sched_dep_time` are similar in having tendency to round off the values to nearest 0.5 Carat (in case of `carat`) or 5/30/60 minutes (in case of `sched_dep_time`). This means when humans record a continuous variable, there is a tendency to round it off to nearest unit, thus leading to a distribution which shows spikes at fixed intervals.

data("diamonds")

diamonds |>
  ggplot(aes(x = carat)) +
  geom_histogram(bins = 500) +
  coord_cartesian(xlim = c(0, 3)) +
  theme_minimal() +
  labs(x = "Carat of Diamond", y = "Number of Diamonds",
       title = "Carats of diamonds are mostly in multiples of 0.5",
       subtitle = "This suggests observations recorders' bias in propensity to round off carat to nearest 0.5") +
  scale_x_continuous(breaks = seq(0, 3, 0.5))

f5 = flights |>
  mutate(sched_dep_time = hour + minute/60)

f5 |>
  ggplot(aes(sched_dep_time)) +
  geom_histogram(bins = 1000) +
  coord_cartesian(xlim = c(5, 24)) +
  theme_minimal() +
  labs(x = "Scheduled departure time (in hr)", 
       y = "Number of Flights",
       title = "Scheduled Departure times are geenrally in multiples of 5, 30 or 60 minutes",
       subtitle = "This suggests propensity to round off scheduled departure time to nearest hour, or atleast 5 minutes") +
  scale_x_continuous(breaks = seq(5, 24, 1))

#7. Confirm our hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed.

# A two-sided t-test of the comparison of these two proportions has a p-value of \< 0.001, thus, there is a significant difference between the two proportions. Thus, we conclude that flights with scheduled departure times between 20-30 min and 50-60 min have a significantly lesser delay than other flights.

f7 = flights |>
  mutate(
    sdt_20_30 = (minute >= 20 & minute <= 30),
    sdt_50_60 = (minute >= 50 & minute <= 60),
    sdt_round_times = (sdt_20_30 | sdt_50_60),
    was_delayed = (dep_delay < 0),
    .keep = "used"
  )

f7 |>
  group_by(sdt_round_times) |>
  summarise(prop_delayed = mean(was_delayed, na.rm = TRUE),
            n = n())

f8test = prop.test(
  x = c(113092, 75098),
  n = c(197298, 139478)
)

f8test
f8test$p.value

## 17.4.4 Exercises

#1. Explain days(!overnight) and days(overnight) to someone who has just started learning R. What is the key fact you need to know?
# In `R`, `TRUE` is represented numerically as 1, and `FALSE` is represented numerically as 0. Thus, `days(!overnight)` is equal to `days(0)`, i.e., a time period of zero days, for an overnight flight, and 1 day for a regular flight.

# On the other hand, `days(overnight)` is equal to `days(1)`, i.e., a time period of 1 day, for an overnight flight, and 0 day for a regular flight.

# The key fact that we need to know is whether the value of `overnight` is `TRUE` or `FALSE` , i.e., whether the flight is an overnight one or not. And, the fact that `R` treats `TRUE` as 1, and `FALSE` as 0.

#2. Create a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.

seq(from = ymd("2015-01-01"),
    to   = ymd("2015-12-31"),
    by   = "1 month")

seq(from = ymd(paste0(year(today()), "-01-01")),
    to   = ymd(paste0(year(today()), "-12-31")),
    by   = "1 month")

#3. Write a function that given your birthday (as a date), returns how old you are in years.

find_age = function(birthday){
  age = floor((interval(parse_date(birthday), today()))/years(1))
  
  cat("You are", age, "years old.")
}

find_age("1991-09-24")

#4. Why can’t (today() %--% (today() + years(1))) / months(1) work?

# This expression should not work because the interval although the interval `(today() %--% (today() + years(1)))` can be computed; but, the duration of `months(1)` is not defined, since, each month is different in length - ranging from 28 days to 31 days. Hence, there is no clear accurate answer to the expression.

(today() %--% (today() + years(1))) / months(1)

```
